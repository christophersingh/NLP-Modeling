{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import math\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticle(url,tag):\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "        html = response.read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "    except:\n",
    "        print (\"Unexpected error:\", sys.exc_info()[1])\n",
    "        return (None,None)\n",
    "    \n",
    "    if (soup is None):\n",
    "        return (None, None)\n",
    "    \n",
    "    article = \"\"\n",
    "    \n",
    "    # Finding the tag within the HTML\n",
    "    if (soup.find_all(tag) is not None):\n",
    "        article = ''.join(map(lambda p: p.text, soup.find_all(tag)))\n",
    "        # Finding the paragraphs <p> tag\n",
    "        #soup_p = BeautifulSoup(article, \"html.parser\")\n",
    "        #if (soup_p.find_all('p') is not None):\n",
    "        #    article = ''.join(map(lambda p: p.text, soup_p.find_all('p')))\n",
    "            \n",
    "    return article, soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining all NLP functions Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(corpus):\n",
    "    punctuations = \".,\\\"-\\\\/#!?$%\\^&\\*;:{}=\\-_'~()\"    \n",
    "    filtered_corpus = [token for token in corpus if (not token in punctuations)]\n",
    "    return filtered_corpus\n",
    "\n",
    "def apply_stopwording(corpus, min_len):\n",
    "    filtered_corpus = [token for token in corpus if (not token in stopwords.words('english') and len(token)>min_len)]\n",
    "    return filtered_corpus\n",
    "\n",
    "def apply_stemming(corpus):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    normalized_corpus = [stemmer.stem(token) for token in corpus]\n",
    "    return normalized_corpus\n",
    "\n",
    "def apply_lemmatization(corpus):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    normalized_corpus = [lemmatizer.lemmatize(token) for token in corpus]\n",
    "    return normalized_corpus\n",
    "\n",
    "def getCollocations(text, min_freq):\n",
    "    bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(text)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    collocations = finder.nbest(bigrams.pmi, 20)\n",
    "    return collocations\n",
    "\n",
    "def getSummary(article, min_freq):\n",
    "    sentences = sent_tokenize(article)\n",
    "    #tokens = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    tokens = nltk.word_tokenize(article.lower())\n",
    "    doc = nltk.Text(tokens)\n",
    "    doc_clean = nltk.Text(apply_lemmatization(apply_stopwording(remove_punctuation(doc), 3)))\n",
    "    collocations = getCollocations(doc_clean,min_freq)\n",
    "\n",
    "    summary = []\n",
    "    for c in collocations:\n",
    "        for sentence in sentences:\n",
    "            term1 = c[0]\n",
    "            term2 = c[1]\n",
    "            term = c[0]+' '+c[1]\n",
    "            if (term in sentence):\n",
    "                #Found the sentence. Add only if not already in the summary\n",
    "                if (sentence not in summary):\n",
    "                    summary.append(sentence)\n",
    "                \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.washingtonpost.com/opinions/on-gun-violence-we-are-a-failed-state/2018/02/18/88ecf09a-137a-11e8-9065-e55346f6de81_story.html\"\n",
    "tag =\"article\"\n",
    "article, title = getArticle(url,tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences: 11\n",
      "1 - On gun violence, the United States has become a corrupt failed state.\n",
      "2 - In corrupt failed states, politics is about lying and misdirection.\n",
      "3 - At the heart of our political system’s failure to address the epidemic of violence is the Republican Party’s decision to become a paid agent of the gun manufacturers’ lobby.\n",
      "4 - No one wants our political system to fail more than Russian President Vladimir Putin does, and our powerlessness on guns hardly enhances our democracy’s image to the world.\n",
      "5 - In no other country are the words “mental health” so empty.\n",
      "6 - He told us: “We are committed to working with state and local leaders to help secure our schools and tackle the difficult issue of mental health.”  Trump’s speech, as Vox’s German Lopez observed, was “one giant lie by omission.” Those 17 people were killed by an AR-15 rifle, not by a knife or a sword or a bomb.\n",
      "7 - Yes, and if Trump cared so much about mental health, he wouldn’t be proposing a $250 billion cut over a decade in Medicaid, which pays for more than 25 percent of the nation’s mental health care.\n",
      "8 - Memo to the media: Stop saying in somber, serious tones that we must do more about mental health.\n",
      "9 - And it’s no wonder that Trump decided he cared so much about mental health.\n",
      "10 - Vox also noted that people with mental illness are more likely to be the victims, not the perpetrators, of violence.\n",
      "11 - We should not have to point out over and over that while mental illness exists everywhere, other countries do not have killing sprees comparable to ours.\n"
     ]
    }
   ],
   "source": [
    "summary = getSummary(article,2)\n",
    "print (\"# of sentences: \"+str(len(summary)))\n",
    "index = 1\n",
    "for sentence in summary:\n",
    "    print (\"%s - %s\" % (index,sentence))\n",
    "    index+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
